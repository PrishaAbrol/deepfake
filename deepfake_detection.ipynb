{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNj6/yUjF6R2cU1wRFv73OG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrishaAbrol/deepfake/blob/main/deepfake_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9JmBtCNilZR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm_notebook\n",
        "%matplotlib inline\n",
        "import cv2 as cv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_FOLDER = '../input/deepfake-detection-challenge'\n",
        "TRAIN_SAMPLE_FOLDER = 'train_sample_videos'\n",
        "TEST_FOLDER = 'test_videos'\n",
        "\n",
        "print(f\"Train samples: {len(os.listdir(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER)))}\")\n",
        "print(f\"Test samples: {len(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER)))}\")"
      ],
      "metadata": {
        "id": "0Gi1RAbiimZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FACE_DETECTION_FOLDER = '../input/haar-cascades-for-face-detection'\n",
        "print(f\"Face detection resources: {os.listdir(FACE_DETECTION_FOLDER)}\")"
      ],
      "metadata": {
        "id": "DCxBELtUipHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_list = list(os.listdir(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER)))\n",
        "ext_dict = []\n",
        "for file in train_list:\n",
        "    file_ext = file.split('.')[1]\n",
        "    if (file_ext not in ext_dict):\n",
        "        ext_dict.append(file_ext)\n",
        "print(f\"Extensions: {ext_dict}\")"
      ],
      "metadata": {
        "id": "z6sxeuVdirzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file_ext in ext_dict:\n",
        "    print(f\"Files with extension `{file_ext}`: {len([file for file in train_list if  file.endswith(file_ext)])}\")"
      ],
      "metadata": {
        "id": "iRUuJr6RisHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_list = list(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER)))\n",
        "ext_dict = []\n",
        "for file in test_list:\n",
        "    file_ext = file.split('.')[1]\n",
        "    if (file_ext not in ext_dict):\n",
        "        ext_dict.append(file_ext)\n",
        "print(f\"Extensions: {ext_dict}\")\n",
        "for file_ext in ext_dict:\n",
        "    print(f\"Files with extension `{file_ext}`: {len([file for file in train_list if  file.endswith(file_ext)])}\")"
      ],
      "metadata": {
        "id": "ynA8W2lcit3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_file = [file for file in train_list if  file.endswith('json')][0]\n",
        "print(f\"JSON file: {json_file}\")"
      ],
      "metadata": {
        "id": "XU8O23ZFivcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_meta_from_json(path):\n",
        "    df = pd.read_json(os.path.join(DATA_FOLDER, path, json_file))\n",
        "    df = df.T\n",
        "    return df\n",
        "\n",
        "meta_train_df = get_meta_from_json(TRAIN_SAMPLE_FOLDER)\n",
        "meta_train_df.head()"
      ],
      "metadata": {
        "id": "JH4gTOSliw-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_data(meta_train_df)"
      ],
      "metadata": {
        "id": "OEjNPrwbizI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_data(meta_train_df.loc[meta_train_df.label=='REAL'])"
      ],
      "metadata": {
        "id": "WQdkP_OIi06P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unique_values(data):\n",
        "    total = data.count()\n",
        "    tt = pd.DataFrame(total)\n",
        "    tt.columns = ['Total']\n",
        "    uniques = []\n",
        "    for col in data.columns:\n",
        "        unique = data[col].nunique()\n",
        "        uniques.append(unique)\n",
        "    tt['Uniques'] = uniques\n",
        "    return(np.transpose(tt))"
      ],
      "metadata": {
        "id": "9VtstM8ii2sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_values(meta_train_df)"
      ],
      "metadata": {
        "id": "qvLNz4X4i5AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def most_frequent_values(data):\n",
        "    total = data.count()\n",
        "    tt = pd.DataFrame(total)\n",
        "    tt.columns = ['Total']\n",
        "    items = []\n",
        "    vals = []\n",
        "    for col in data.columns:\n",
        "        itm = data[col].value_counts().index[0]\n",
        "        val = data[col].value_counts().values[0]\n",
        "        items.append(itm)\n",
        "        vals.append(val)\n",
        "    tt['Most frequent item'] = items\n",
        "    tt['Frequence'] = vals\n",
        "    tt['Percent from total'] = np.round(vals / total * 100, 3)\n",
        "    return(np.transpose(tt))"
      ],
      "metadata": {
        "id": "CTHlunuwi6rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_frequent_values(meta_train_df)"
      ],
      "metadata": {
        "id": "wPRGKttqi9Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_count(feature, title, df, size=1):\n",
        "    '''\n",
        "    Plot count of classes / feature\n",
        "    param: feature - the feature to analyze\n",
        "    param: title - title to add to the graph\n",
        "    param: df - dataframe from which we plot feature's classes distribution\n",
        "    param: size - default 1.\n",
        "    '''\n",
        "    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n",
        "    total = float(len(df))\n",
        "    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set3')\n",
        "    g.set_title(\"Number and percentage of {}\".format(title))\n",
        "    if(size > 2):\n",
        "        plt.xticks(rotation=90, size=8)\n",
        "    for p in ax.patches:\n",
        "        height = p.get_height()\n",
        "        ax.text(p.get_x()+p.get_width()/2.,\n",
        "                height + 3,\n",
        "                '{:1.2f}%'.format(100*height/total),\n",
        "                ha=\"center\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VtD2OYYci_Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_count('split', 'split (train)', meta_train_df)"
      ],
      "metadata": {
        "id": "fS7moHLYjAhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta = np.array(list(meta_train_df.index))\n",
        "storage = np.array([file for file in train_list if  file.endswith('mp4')])\n",
        "print(f\"Metadata: {meta.shape[0]}, Folder: {storage.shape[0]}\")\n",
        "print(f\"Files in metadata and not in folder: {np.setdiff1d(meta,storage,assume_unique=False).shape[0]}\")\n",
        "print(f\"Files in folder and not in metadata: {np.setdiff1d(storage,meta,assume_unique=False).shape[0]}\")"
      ],
      "metadata": {
        "id": "tuyi89LDjCzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_train_sample_video = list(meta_train_df.loc[meta_train_df.label=='FAKE'].sample(3).index)\n",
        "fake_train_sample_video"
      ],
      "metadata": {
        "id": "Ga6-1nXDjEnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_image_from_video(video_path):\n",
        "    '''\n",
        "    input: video_path - path for video\n",
        "    process:\n",
        "    1. perform a video capture from the video\n",
        "    2. read the image\n",
        "    3. display the image\n",
        "    '''\n",
        "    capture_image = cv.VideoCapture(video_path)\n",
        "    ret, frame = capture_image.read()\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(111)\n",
        "    frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
        "    ax.imshow(frame)"
      ],
      "metadata": {
        "id": "YTeTPWTcjJkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for video_file in fake_train_sample_video:\n",
        "    display_image_from_video(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file))"
      ],
      "metadata": {
        "id": "UCbyELHHjLtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_train_sample_video = list(meta_train_df.loc[meta_train_df.label=='REAL'].sample(3).index)\n",
        "real_train_sample_video"
      ],
      "metadata": {
        "id": "sICRPaT5jOsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for video_file in real_train_sample_video:\n",
        "    display_image_from_video(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file))"
      ],
      "metadata": {
        "id": "98asFnTyjQdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_train_df['original'].value_counts()[0:5]"
      ],
      "metadata": {
        "id": "pLFSUQInjSpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_image_from_video_list(video_path_list, video_folder=TRAIN_SAMPLE_FOLDER):\n",
        "    '''\n",
        "    input: video_path_list - path for video\n",
        "    process:\n",
        "    0. for each video in the video path list\n",
        "        1. perform a video capture from the video\n",
        "        2. read the image\n",
        "        3. display the image\n",
        "    '''\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2,3,figsize=(16,8))\n",
        "    # we only show images extracted from the first 6 videos\n",
        "    for i, video_file in enumerate(video_path_list[0:6]):\n",
        "        video_path = os.path.join(DATA_FOLDER, video_folder,video_file)\n",
        "        capture_image = cv.VideoCapture(video_path)\n",
        "        ret, frame = capture_image.read()\n",
        "        frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
        "        ax[i//3, i%3].imshow(frame)\n",
        "        ax[i//3, i%3].set_title(f\"Video: {video_file}\")\n",
        "        ax[i//3, i%3].axis('on')"
      ],
      "metadata": {
        "id": "hVsOos5NjVAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "same_original_fake_train_sample_video = list(meta_train_df.loc[meta_train_df.original=='meawmsgiti.mp4'].index)\n",
        "display_image_from_video_list(same_original_fake_train_sample_video)"
      ],
      "metadata": {
        "id": "MVmfs4hDjXJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "same_original_fake_train_sample_video = list(meta_train_df.loc[meta_train_df.original=='kgbkktcjxf.mp4'].index)\n",
        "display_image_from_video_list(same_original_fake_train_sample_video)"
      ],
      "metadata": {
        "id": "Y7aKI1J4jZX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_videos = pd.DataFrame(list(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER))), columns=['video'])"
      ],
      "metadata": {
        "id": "6soRbYjJjbni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_videos.head()"
      ],
      "metadata": {
        "id": "1laM-bG5jdr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_image_from_video(os.path.join(DATA_FOLDER, TEST_FOLDER, test_videos.iloc[0].video))"
      ],
      "metadata": {
        "id": "gVeZswaMjfWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_image_from_video_list(test_videos.sample(6).video, TEST_FOLDER)"
      ],
      "metadata": {
        "id": "abvlW7_QjhPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ObjectDetector():\n",
        "    '''\n",
        "    Class for Object Detection\n",
        "    '''\n",
        "    def __init__(self,object_cascade_path):\n",
        "        '''\n",
        "        param: object_cascade_path - path for the *.xml defining the parameters for {face, eye, smile, profile}\n",
        "        detection algorithm\n",
        "        source of the haarcascade resource is: https://github.com/opencv/opencv/tree/master/data/haarcascades\n",
        "        '''\n",
        "\n",
        "        self.objectCascade=cv.CascadeClassifier(object_cascade_path)\n",
        "\n",
        "\n",
        "    def detect(self, image, scale_factor=1.3,\n",
        "               min_neighbors=5,\n",
        "               min_size=(20,20)):\n",
        "        '''\n",
        "        Function return rectangle coordinates of object for given image\n",
        "        param: image - image to process\n",
        "        param: scale_factor - scale factor used for object detection\n",
        "        param: min_neighbors - minimum number of parameters considered during object detection\n",
        "        param: min_size - minimum size of bounding box for object detected\n",
        "        '''\n",
        "        rects=self.objectCascade.detectMultiScale(image,\n",
        "                                                scaleFactor=scale_factor,\n",
        "                                                minNeighbors=min_neighbors,\n",
        "                                                minSize=min_size)\n",
        "        return rects"
      ],
      "metadata": {
        "id": "wK3g4x4mjjA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frontal_cascade_path= os.path.join(FACE_DETECTION_FOLDER,'haarcascade_frontalface_default.xml')\n",
        "eye_cascade_path= os.path.join(FACE_DETECTION_FOLDER,'haarcascade_eye.xml')\n",
        "profile_cascade_path= os.path.join(FACE_DETECTION_FOLDER,'haarcascade_profileface.xml')\n",
        "smile_cascade_path= os.path.join(FACE_DETECTION_FOLDER,'haarcascade_smile.xml')\n",
        "\n",
        "#Detector object created\n",
        "# frontal face\n",
        "fd=ObjectDetector(frontal_cascade_path)\n",
        "# eye\n",
        "ed=ObjectDetector(eye_cascade_path)\n",
        "# profile face\n",
        "pd=ObjectDetector(profile_cascade_path)\n",
        "# smile\n",
        "sd=ObjectDetector(smile_cascade_path)"
      ],
      "metadata": {
        "id": "KtI7LjNIjlko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_objects(image, scale_factor, min_neighbors, min_size):\n",
        "    '''\n",
        "    Objects detection function\n",
        "    Identify frontal face, eyes, smile and profile face and display the detected objects over the image\n",
        "    param: image - the image extracted from the video\n",
        "    param: scale_factor - scale factor parameter for `detect` function of ObjectDetector object\n",
        "    param: min_neighbors - min neighbors parameter for `detect` function of ObjectDetector object\n",
        "    param: min_size - minimum size parameter for f`detect` function of ObjectDetector object\n",
        "    '''\n",
        "\n",
        "    image_gray=cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "\n",
        "    eyes=ed.detect(image_gray,\n",
        "                   scale_factor=scale_factor,\n",
        "                   min_neighbors=min_neighbors,\n",
        "                   min_size=(int(min_size[0]/2), int(min_size[1]/2)))\n",
        "\n",
        "    for x, y, w, h in eyes:\n",
        "        #detected eyes shown in color image\n",
        "        cv.circle(image,(int(x+w/2),int(y+h/2)),(int((w + h)/4)),(0, 0,255),3)\n",
        "\n",
        "    # deactivated due to many false positive\n",
        "    #smiles=sd.detect(image_gray,\n",
        "    #               scale_factor=scale_factor,\n",
        "    #               min_neighbors=min_neighbors,\n",
        "    #               min_size=(int(min_size[0]/2), int(min_size[1]/2)))\n",
        "\n",
        "    #for x, y, w, h in smiles:\n",
        "    #    #detected smiles shown in color image\n",
        "    #    cv.rectangle(image,(x,y),(x+w, y+h),(0, 0,255),3)\n",
        "\n",
        "\n",
        "    profiles=pd.detect(image_gray,\n",
        "                   scale_factor=scale_factor,\n",
        "                   min_neighbors=min_neighbors,\n",
        "                   min_size=min_size)\n",
        "\n",
        "    for x, y, w, h in profiles:\n",
        "        #detected profiles shown in color image\n",
        "        cv.rectangle(image,(x,y),(x+w, y+h),(255, 0,0),3)\n",
        "\n",
        "    faces=fd.detect(image_gray,\n",
        "                   scale_factor=scale_factor,\n",
        "                   min_neighbors=min_neighbors,\n",
        "                   min_size=min_size)\n",
        "\n",
        "    for x, y, w, h in faces:\n",
        "        #detected faces shown in color image\n",
        "        cv.rectangle(image,(x,y),(x+w, y+h),(0, 255,0),3)\n",
        "\n",
        "    # image\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(111)\n",
        "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
        "    ax.imshow(image)"
      ],
      "metadata": {
        "id": "AvdRc4XdjnhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_image_objects(video_file, video_set_folder=TRAIN_SAMPLE_FOLDER):\n",
        "    '''\n",
        "    Extract one image from the video and then perform face/eyes/smile/profile detection on the image\n",
        "    param: video_file - the video from which to extract the image from which we extract the face\n",
        "    '''\n",
        "    video_path = os.path.join(DATA_FOLDER, video_set_folder,video_file)\n",
        "    capture_image = cv.VideoCapture(video_path)\n",
        "    ret, frame = capture_image.read()\n",
        "    #frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
        "    detect_objects(image=frame,\n",
        "            scale_factor=1.3,\n",
        "            min_neighbors=5,\n",
        "            min_size=(50, 50))"
      ],
      "metadata": {
        "id": "6qFEKHqTjrHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "same_original_fake_train_sample_video = list(meta_train_df.loc[meta_train_df.original=='kgbkktcjxf.mp4'].index)\n",
        "for video_file in same_original_fake_train_sample_video[1:4]:\n",
        "    print(video_file)\n",
        "    extract_image_objects(video_file)"
      ],
      "metadata": {
        "id": "cNOwC7_Ljs4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_subsample_video = list(meta_train_df.sample(3).index)\n",
        "for video_file in train_subsample_video:\n",
        "    print(video_file)\n",
        "    extract_image_objects(video_file)"
      ],
      "metadata": {
        "id": "OHlS-9DKjuk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subsample_test_videos = list(test_videos.sample(3).video)\n",
        "for video_file in subsample_test_videos:\n",
        "    print(video_file)\n",
        "    extract_image_objects(video_file, TEST_FOLDER)"
      ],
      "metadata": {
        "id": "c-7cbjRkjwiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_videos = list(meta_train_df.loc[meta_train_df.label=='FAKE'].index)"
      ],
      "metadata": {
        "id": "ZYN5Gg6rj2K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "def play_video(video_file, subset=TRAIN_SAMPLE_FOLDER):\n",
        "    '''\n",
        "    Display video\n",
        "    param: video_file - the name of the video file to display\n",
        "    param: subset - the folder where the video file is located (can be TRAIN_SAMPLE_FOLDER or TEST_Folder)\n",
        "    '''\n",
        "    video_url = open(os.path.join(DATA_FOLDER, subset,video_file),'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(video_url).decode()\n",
        "    return HTML(\"\"\"<video width=500 controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)"
      ],
      "metadata": {
        "id": "gMW-LSWvj37g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_video(fake_videos[0])"
      ],
      "metadata": {
        "id": "syzsA3FYj4eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_video(fake_videos[1])"
      ],
      "metadata": {
        "id": "ki1FTsOLj6PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_video(fake_videos[2])"
      ],
      "metadata": {
        "id": "7ZaDOCXej-2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_video(fake_videos[3])"
      ],
      "metadata": {
        "id": "OtiMau0CkAn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_video(fake_videos[4])"
      ],
      "metadata": {
        "id": "_YXcVllfkCFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_video(fake_videos[5])"
      ],
      "metadata": {
        "id": "-NLao8mokDac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_video(fake_videos[10])"
      ],
      "metadata": {
        "id": "DfHBgEQtkFIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_video(fake_videos[12])"
      ],
      "metadata": {
        "id": "tNp-uRiCkJk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_video(fake_videos[15])"
      ],
      "metadata": {
        "id": "Y1ORUXJEkNwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_video(fake_videos[18])"
      ],
      "metadata": {
        "id": "7ZXCQ0xWkOGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print\"Here's a chatbot for extra info:\"\n",
        "import dialogflow_v2 as dialogflow\n",
        "import os\n",
        "\n",
        "# Set the path to your Google Cloud service account key file\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"path-to-your-service-account-file.json\"\n",
        "\n",
        "def detect_intent_texts(project_id, session_id, text, language_code):\n",
        "    session_client = dialogflow.SessionsClient()\n",
        "    session = session_client.session_path(project_id, session_id)\n",
        "\n",
        "    text_input = dialogflow.types.TextInput(text=text, language_code=language_code)\n",
        "    query_input = dialogflow.types.QueryInput(text=text_input)\n",
        "\n",
        "    response = session_client.detect_intent(session=session, query_input=query_input)\n",
        "\n",
        "    return response.query_result.fulfillment_text\n",
        "\n",
        "# Example usage\n",
        "project_id = 'your-dialogflow-project-id'\n",
        "session_id = '123456'  # Random unique ID for each session\n",
        "text = 'Hello, how can you help me?'\n",
        "language_code = 'en'\n",
        "\n",
        "response = detect_intent_texts(project_id, session_id, text, language_code)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "fifoHyTqkz5W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}